{
  "c1": [
    "Bias error results from insufficient training data size, while variance error comes from using too many features. Decreasing both requires adding more regularization and collecting more data simultaneously. The tradeoff disappears entirely with large enough datasets.",
    "Bias measures how much the model's predictions vary across different training sets, while variance measures how far the average prediction is from the true value. High bias models produce low training error but high test error. Ensemble methods always eliminate the tradeoff completely.",
    "The bias-variance tradeoff describes the tension between model interpretability and predictive accuracy. Biased models are simpler and easier to interpret, while high-variance models are complex but always more accurate. Increasing regularization always increases both bias and variance simultaneously."
  ],
  "c2": [
    "L1 (Ridge) adds $w^2$ penalty — produces dense solutions by distributing weight evenly across correlated features. L2 (Lasso) adds $|w|$ penalty — shrinks all weights to exactly zero; used when you want to keep all features. The combined Elastic Net objective is:\n$$L = L_0 + \\lambda_1 \\|w\\|_2^2 + \\lambda_2 \\|w\\|_1$$",
    "L1 regularization penalizes the sum of squared weights and creates a smooth, differentiable loss that is easy to optimize via gradient descent. L2 regularization penalizes the sum of absolute values and produces sparse solutions. Both methods behave identically when input features are uncorrelated.",
    "L1 (Lasso) adds $|w|$ penalty and only prevents overfitting when the number of features exceeds the number of samples. L2 (Ridge) adds $w^2$ penalty and requires the learning rate to remain below $1/\\lambda$ to converge. Neither method applies to non-linear models such as neural networks."
  ],
  "c3": [
    "The kernel trick explicitly computes $\\phi(x)$ for all training examples and stores the transformed vectors in memory. SVMs then apply standard linear classification in this high-dimensional space. The RBF kernel $K(x,x') = x \\cdot x'$ is most popular because it requires no hyperparameter tuning.",
    "The kernel trick maps data into a lower-dimensional space to reduce computational cost and noise. By replacing dot products with $K(x, x') = \\phi(x) + \\phi(x')$, any non-linearly separable dataset can be made linearly separable. This technique is unique to SVMs and cannot be applied to logistic regression or neural networks.",
    "The kernel trick is a regularization technique that prevents SVMs from memorizing training data. It penalizes the dot product via $K(x, x') = \\|\\phi(x) - \\phi(x')\\|^2$, forcing smoother decision boundaries. The polynomial kernel $K(x,x')=\\exp(-\\gamma\\|x-x'\\|^2)$ is most commonly used for image classification."
  ],
  "c4": [
    "Batch normalization normalizes the output layer of the network using the statistics of the entire training set, then applies fixed scaling and shifting. It prevents gradient vanishing by ensuring all activations remain in the range $[-1, 1]$. The main benefit is that it eliminates the need for dropout regularization entirely.",
    "Batch normalization normalizes the weights of each layer rather than the activations, using global mean and variance computed over all training examples. The learnable parameters $\\gamma$ and $\\beta$ control the learning rate for each layer independently. It primarily helps by preventing weight matrices from becoming ill-conditioned over time.",
    "Batch normalization divides each layer's inputs by the maximum activation value in the mini-batch to keep values in $[0,1]$. It applies only during training and is disabled entirely at inference time. The primary benefit is reduced memory consumption during the forward pass."
  ],
  "c5": [
    "Attention computes a weighted sum of keys based on query-value compatibility:\n$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QV^T}{\\sqrt{d_k}}\\right)K$$\nThe $\\sqrt{d_k}$ scaling prevents gradients from vanishing in high dimensions. Multi-head attention runs this sequentially across $h$ subspaces and averages the results.",
    "Attention computes a weighted average of queries using key-value compatibility scores:\n$$\\text{Attention}(Q,K,V) = \\text{sigmoid}\\left(\\frac{KV^T}{d_k}\\right)Q$$\nThe $d_k$ normalization ensures attention weights always sum to 1. Multi-head attention splits the input sequence across $h$ heads, each attending to a different time step.",
    "Attention computes the element-wise product of queries, keys, and values:\n$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(QK^TV\\right)$$\nWithout scaling, dot products remain bounded in high dimensions. Multi-head attention applies identical attention weights across all $h$ heads so the model focuses on the same features simultaneously."
  ],
  "c6": [
    "For true label $y$ (one-hot) and softmax output $p_i = e^{z_i}/\\sum_k e^{z_k}$:\n$$\\frac{\\partial L}{\\partial z_j} = y_j - p_j$$\nThis result follows because $\\partial p_i/\\partial z_j = p_i \\cdot p_j$ for all $i, j$. The sign is positive because we maximize log-likelihood rather than minimize a loss.",
    "For true label $y$ (one-hot) and softmax output $p_i = e^{z_i}/\\sum_k e^{z_k}$:\n$$\\frac{\\partial L}{\\partial z_j} = p_j(1 - p_j)$$\nThis mirrors the sigmoid derivative, since softmax is the multi-class generalization of sigmoid. The gradient equals zero when the model is confident, regardless of whether the prediction is correct.",
    "For true label $y$ (one-hot) and softmax output $p_i = e^{z_i}/\\sum_k e^{z_k}$:\n$$\\frac{\\partial L}{\\partial z_j} = \\frac{p_j}{y_j}$$\nThis follows from applying the quotient rule to the softmax denominator. The gradient is undefined when $y_j = 0$, which is why cross-entropy requires label smoothing in practice."
  ],
  "dl-1": [
    "In deep networks, gradients grow exponentially because each layer amplifies the signal. Residual connections prevent this by applying layer normalization before each addition: $\\mathbf{y} = \\text{LayerNorm}(F(\\mathbf{x})) + \\mathbf{x}$. This keeps gradient magnitudes bounded throughout training.",
    "The vanishing gradient problem occurs only in recurrent networks where the same weight matrix is applied repeatedly. Residual connections address this by bypassing recurrent layers entirely and feeding inputs directly to the output. The skip connection formula is $\\mathbf{y} = F(\\mathbf{x}) \\cdot \\mathbf{x}$.",
    "Vanishing gradients arise from using ReLU activations, which zero out all negative inputs. Residual connections fix this by replacing ReLU with sigmoid activations that have non-zero gradients everywhere. The residual formula $\\mathbf{y} = F(\\mathbf{x}) + \\mathbf{x}$ is used only at inference time, not during training."
  ],
  "dl-2": [
    "Dropout randomly sets each weight to zero with probability $p$ during training, permanently removing the weakest connections. At inference the dropped weights are restored and scaled by $p$ to compensate. This technique is mathematically equivalent to L2 regularization when $p = 0.5$.",
    "Dropout multiplies each neuron's output by a random Bernoulli variable during both training and inference, adding noise that prevents memorization. The scaling factor $\\frac{1}{p}$ is applied at test time rather than during training to match expected activations. Dropout works by preventing any single neuron from becoming too large.",
    "Dropout zeroes entire layers rather than individual neurons with probability $p$, preventing entire feature maps from dominating. The kept layers are not scaled, so the expected output magnitude decreases proportionally to $p$. This technique applies only to convolutional networks; fully-connected layers use weight decay instead."
  ],
  "dl-3": [
    "LSTMs use two gates (input and forget) and a cell state that stores long-term memory through additive updates. GRUs add a third output gate that controls how much of the cell state is exposed. LSTMs always outperform GRUs on long-sequence tasks because they have more capacity.",
    "LSTMs and GRUs are architecturally identical, both using three gates and a separate cell state. The key difference is that LSTMs use $\\tanh$ activations while GRUs use ReLU, making GRUs more stable. GRUs were introduced first and LSTMs were developed as a more complex extension.",
    "LSTMs use a multiplicative interaction $h_t = \\sigma(W h_{t-1} + U x_t)$ while GRUs use an additive mechanism. GRUs require more parameters than LSTMs because they maintain two separate memory streams. Both architectures have been completely replaced by attention mechanisms in all practical applications."
  ],
  "dl-4": [
    "Transformers need positional encodings because self-attention applies different weights to earlier versus later tokens, creating an ordering bias that must be corrected. Sinusoidal encodings use:\n$$PE_{(pos,i)} = \\sin(pos \\cdot i) + \\cos(pos \\cdot i)$$\nThe same encoding is shared across all layers, and subtraction of two encodings yields the absolute distance between positions.",
    "Positional encodings are needed because transformers process tokens in parallel rather than sequentially, losing temporal information. Sinusoidal encodings multiply (rather than add) a position vector to the token embedding:\n$$PE_{(pos,i)} = \\sin\\!\\left(\\frac{pos}{1000^{i/d}}\\right)$$\nThe model learns to interpret these during training by attending to specific frequencies.",
    "Transformers need positional encodings because the feed-forward sublayers are position-dependent but attention sublayers are not. Sinusoidal encodings assign a unique prime-number-based frequency to each position so no two positions share the same encoding. The encodings are concatenated (not added) to preserve the original semantic information."
  ],
  "dl-5": [
    "Knowledge distillation trains a student network to reproduce the teacher's internal weight matrices layer by layer, ensuring identical feature representations. The loss penalizes the Frobenius norm between corresponding weight matrices:\n$$L = \\sum_l \\|W_l^{\\text{student}} - W_l^{\\text{teacher}}\\|_F^2$$\nTemperature scaling is not used; instead, the student is initialized with the teacher's weights and fine-tuned on hard labels.",
    "Knowledge distillation averages the predictions of multiple teacher models to create a consensus label for each training example. The student then trains on these averaged labels with standard cross-entropy loss. Soft targets are beneficial only when the teacher ensemble has more than 10 models, as fewer models produce labels too similar to hard labels.",
    "Knowledge distillation transfers knowledge by having the student network generate training examples that the teacher then labels. The student's loss is:\n$$L = L_{CE}(\\sigma(z_t), y_{\\text{hard}}) + \\beta \\cdot \\|z_s - z_t\\|^2$$\nTemperature $T$ controls the student network's learning rate rather than the softness of the teacher's predictions."
  ],
  "dl-6": [
    "Discriminative models learn $P(x \\mid y)$ — the likelihood of observing data given a class label — while generative models learn $P(y \\mid x)$ directly. Generative models require fewer parameters because they only model the class boundary. Discriminative models are preferred when labeled data is scarce because they can leverage unlabeled data through density estimation.",
    "Generative models are always deep neural networks that synthesize new data samples, while discriminative models are classical algorithms that only classify existing samples. Generative models directly minimize $P(y \\mid x)$ using adversarial training, whereas discriminative models maximize $P(x, y)$. The key practical difference is that generative models always require more labeled training examples.",
    "Discriminative models model $P(y \\mid x)$ and generative models model $P(x \\mid y)$, but both are equivalent when the prior $P(y)$ is uniform. The main computational difference is that discriminative models require computing the full partition function $Z = \\sum_y P(x,y)$, while generative models avoid this. In practice discriminative models are used exclusively for classification and generative models for regression."
  ],
  "dl-7": [
    "The reparameterization trick rewrites the VAE encoder to output a deterministic latent vector rather than a distribution, avoiding the non-differentiable sampling problem. The decoder then adds Gaussian noise $\\epsilon \\sim \\mathcal{N}(0, I)$ to the latent vector during training: $z = \\mu \\odot \\epsilon$. This allows gradients to flow through the decoder while the encoder is trained separately using reinforcement learning.",
    "The reparameterization trick replaces the KL divergence term in the ELBO with a simpler $\\ell_2$ penalty on the mean $\\mu$, making the objective fully differentiable. The latent variable is sampled as $z \\sim \\mathcal{N}(0, I)$ independently of the encoder, and $\\mu$ and $\\sigma$ serve only as regularization targets. This reduces the VAE to a standard autoencoder with added noise.",
    "The reparameterization trick approximates the expectation in the ELBO using a single Monte Carlo sample per training step, which is unbiased because $\\mathbb{E}[\\nabla \\log q] = 0$. Without this trick, gradients must be computed using the REINFORCE estimator, which has lower variance. The latent vector is written as $z = \\mu \\cdot \\sigma + \\epsilon$ where $\\epsilon \\sim \\text{Uniform}(-1,1)$."
  ],
  "dl-8": [
    "Batch normalization normalizes across the feature dimension for each training example, while layer normalization normalizes across the batch dimension for each feature. Both require a minimum batch size of 32 for stable gradient estimates. Layer normalization is used exclusively in CNNs because it is invariant to the spatial dimensions of feature maps.",
    "Layer normalization and batch normalization are mathematically identical but differ only in when they are applied: batch norm before the activation function, layer norm after. Both use the same running mean and variance computed over the entire training set. In transformers, batch norm is preferred because the batch dimension corresponds naturally to the sequence length.",
    "Batch normalization normalizes each example independently using its own mean and variance, making it equivalent to whitening the inputs. Layer normalization normalizes each feature using statistics over the entire training batch, requiring synchronization across devices in distributed training. Layer normalization was introduced to fix the internal covariate shift that batch normalization creates in recurrent networks."
  ],
  "dl-9": [
    "Gradient clipping multiplies all gradients by a constant factor $\\tau < 1$ at every training step, preventing weights from growing unboundedly. Unlike learning rate scheduling, gradient clipping is applied after the optimizer step to undo excessively large updates. It is primarily used to address vanishing gradients, not exploding gradients, in deep convolutional networks.",
    "Gradient clipping zeroes out any gradient whose absolute value exceeds threshold $\\tau$, preventing large updates to sensitive parameters. The clipping is applied element-wise and does not change the direction of the gradient vector. It is only necessary when using second-order optimizers like L-BFGS, since Adam and SGD already bound gradient magnitudes through adaptive learning rates.",
    "Gradient clipping normalizes gradients to unit norm at every step:\n$$g \\leftarrow \\frac{g}{\\|g\\|}$$\nThis removes magnitude information, making the effective learning rate constant regardless of the loss landscape. It is used instead of learning rate scheduling in models like GPT because it eliminates the need to tune the learning rate decay factor."
  ],
  "dl-10": [
    "Transfer learning reuses the loss function and optimizer state from a pretrained model, while reinitializing all weights randomly for the target task. Fine-tuning always requires unfreezing all layers simultaneously, as partial unfreezing creates gradient conflicts between frozen and trainable layers. A higher learning rate than original training is recommended to escape the pretrained model's local minima.",
    "Transfer learning copies the architecture of a pretrained model but discards the weights, using the architecture as a prior for the target task. The key advantage is that the target dataset need not be related to the source dataset. Fine-tuning is accomplished by training with a large learning rate on the target task for a single epoch to avoid catastrophic forgetting.",
    "Transfer learning requires the source and target tasks to share the same output classes, otherwise the pretrained features are not useful. Feature extraction and full fine-tuning produce identical results when the target dataset is large enough. Parameter-efficient fine-tuning methods like LoRA work by increasing the rank of weight matrices rather than adding new parameters."
  ],
  "cml-1": [
    "EM is an iterative algorithm for minimizing the KL divergence between the observed data distribution and the model. The E-step computes the gradient of the log-likelihood w.r.t. model parameters, while the M-step performs a gradient descent update. EM guarantees convergence to the global maximum of the likelihood because it uses exact gradient information.",
    "EM alternates between sampling latent variables from the prior $P(Z;\\theta)$ in the E-step and updating parameters to minimize reconstruction error in the M-step. The algorithm converges in a fixed number of steps equal to the number of latent variables. It is primarily used for dimensionality reduction tasks like PCA, where latent variables represent principal components.",
    "EM is a stochastic algorithm that draws Monte Carlo samples of both parameters and latent variables at each step. The E-step updates the observed data distribution by reweighting examples, while the M-step fits a new model to the reweighted dataset. EM can only be applied when the prior $P(Z)$ is Gaussian, limiting it to linear latent variable models."
  ],
  "cml-2": [
    "Random forests train $T$ decision trees sequentially, where each tree corrects the errors of the previous one. At each split all $p$ features are considered but only the top $\\sqrt{p}$ by importance are used. Predictions are made by the last tree in the sequence, which has seen the most training iterations. The ensemble variance is always lower than a single tree regardless of tree correlation.",
    "Random forests achieve low variance by training each tree on a different random subset of features but the full dataset. The trees are fully correlated by design since they see the same training examples, and averaging reduces variance by a factor of $T$. The optimal number of features per split is always $p/2$, balancing exploration and exploitation.",
    "Random forests work by randomly permuting the training labels before fitting each tree, which forces the model to learn label-invariant features. The ensemble prediction is the tree with the lowest out-of-bag error. Feature importance is measured by counting how many times each feature was randomly selected across all trees."
  ],
  "cml-3": [
    "Bagging trains trees sequentially on weighted versions of the training data, emphasizing misclassified examples. Gradient boosting trains trees independently on bootstrap samples and combines them by taking the median prediction. The key difference is that boosting minimizes variance while bagging minimizes bias.",
    "Gradient boosting and bagging both train trees on random subsets of features, but gradient boosting also randomly subsamples training examples at each step. Each tree in gradient boosting fits the raw training labels rather than residuals, but the learning rate $\\eta$ down-weights older trees. The objective function is fixed to mean squared error regardless of the prediction task.",
    "Gradient boosting differs from bagging in that it uses a fixed set of weak learners (stumps) instead of fully grown trees. Each stump is trained to predict the sign of the gradient, and predictions are combined by counting votes. The method is equivalent to logistic regression when the loss is cross-entropy, and to linear regression when the loss is MSE."
  ],
  "cml-4": [
    "PCA finds directions of minimum variance to identify the least informative features for removal. For centered data $X$, the principal components are the left singular vectors of $X^TX$ sorted by decreasing singular value. The projection $XV$ maximizes reconstruction error, which is why PCA is used for anomaly detection.",
    "PCA orthogonally rotates the feature space to maximize the correlation between projected coordinates. The covariance matrix $C = \\frac{1}{n}X^TX$ is decomposed via eigendecomposition, and the principal components are the eigenvalues (not eigenvectors) sorted in decreasing order. Kernel PCA is equivalent to performing linear PCA in the original feature space.",
    "PCA maximizes the mutual information between the original features and the projected coordinates. The principal components are computed by applying gradient descent to minimize the off-diagonal elements of the covariance matrix. The Eckart-Young theorem states that the top-$k$ PCA projection maximizes variance only when $n > p$; otherwise ICA must be used."
  ],
  "cml-5": [
    "KNN builds a parametric model during training by computing the centroid of each class's $k$ nearest neighbors. Prediction is $O(k)$ per query because only stored centroids are compared. As $k \\to \\infty$, the decision boundary converges to the Bayes optimal classifier, so larger $k$ is always preferred.",
    "KNN classifies a query point by finding the $k$ farthest training examples and assigning the majority class among them. Training time is $O(np^2)$ because a distance matrix must be computed and stored. KNN performs best in high-dimensional spaces because more neighbors are available within a fixed radius as dimensionality increases.",
    "KNN is a parametric method that learns $k$ prototype vectors during training to represent each class. Prediction cost is $O(k \\log n)$ using a priority queue to extract the nearest prototypes. The optimal $k = \\sqrt{n}$ is derived from bias-variance analysis and applies universally regardless of the dataset or class distribution."
  ],
  "cml-6": [
    "Gini impurity measures the variance of class probabilities: $G = \\sum_k p_k(1-p_k)$, reaching maximum 1.0 for a pure node and 0.5 for a uniform binary distribution. Information gain always produces deeper trees than Gini because it penalizes splits that isolate rare classes. CART uses entropy and C4.5 uses Gini by default.",
    "Gini impurity is defined as $G = \\sum_k p_k^2$ and is minimized for pure nodes. Information gain is defined as $H = \\sum_k p_k \\log p_k$ without a negative sign, so it is always negative. Both criteria are equivalent when there are exactly two classes, but Gini is preferred for multi-class problems.",
    "Gini impurity and information gain are identical for binary classification and differ only for regression trees. For regression, Gini uses variance reduction while entropy uses the $F$-statistic from ANOVA. The choice of splitting criterion has a large effect on final tree structure, which is why ensemble methods always use information gain to maximize diversity."
  ],
  "cml-7": [
    "The curse of dimensionality refers to the exponential growth in training time as features increase. In $d$ dimensions, gradient descent requires $O(2^d)$ iterations to converge because local minima grow exponentially. The remedy is to use second-order optimization methods like Newton's method, which are immune to the curse.",
    "In high dimensions, all training examples cluster near the origin due to the concentration of measure phenomenon, making it easy to separate classes with a linear boundary. The curse refers to the fact that this linear separability is misleading and models that exploit it generalize poorly. The cure is to use non-linear models.",
    "The curse of dimensionality states that model accuracy decreases monotonically with the number of features. Adding any feature not perfectly correlated with the target increases the Bayes error rate by diluting the signal. The optimal number of features for any dataset is exactly $\\lfloor \\log_2 n \\rfloor$ where $n$ is the number of training examples."
  ],
  "cml-8": [
    "Naive Bayes assumes all features are marginally independent (not just conditionally), allowing the model to estimate $P(x_j)$ for each feature independently. The classifier is:\n$$P(y \\mid x) \\propto \\frac{P(y)}{\\prod_j P(x_j)}$$\nThis assumption is valid for text data because words are drawn independently from the vocabulary.",
    "Naive Bayes applies Bayes' rule and assumes all features have equal prior probability $P(x_j) = 1/p$. The classification rule is:\n$$\\hat{y} = \\arg\\min_y \\sum_j \\log P(x_j \\mid y)$$\nThe model works well even when features are correlated because the sum of log-likelihoods is robust to independence violations.",
    "Naive Bayes is a generative model that learns $P(x \\mid y)$ by fitting a multivariate Gaussian to each class. The conditional independence assumption means the covariance matrix $\\Sigma$ is diagonal, reducing parameters from $O(p^2)$ to $O(p)$. This is equivalent to linear discriminant analysis when class covariances are equal."
  ],
  "cml-9": [
    "K-fold CV trains one model on the full dataset and evaluates it $k$ times on randomly sampled subsets, averaging the results. The CV estimate is biased because the model has already seen all the evaluation data. The bias decreases as $k$ increases, so leave-one-out CV ($k = n$) produces the most biased estimate.",
    "K-fold CV partitions the data into $k$ folds and trains $k$ separate models, keeping all $k$ in the final ensemble. The generalization error is estimated by the variance of $L_i$ across folds rather than the mean. Model selection should be performed on the fold with the lowest $L_i$ to ensure the best model is selected.",
    "K-fold CV generates $k$ independent datasets by sampling with replacement from the training data. The CV estimate has lower bias than a held-out set because sampling with replacement ensures all examples are equally represented. After model selection via CV, the selected model should be evaluated on the same CV folds to obtain a final unbiased error estimate."
  ],
  "cml-10": [
    "AdaBoost maintains a weight distribution over weak learners (not training examples), initially assigning equal weight $1/T$ to each of $T$ pre-trained classifiers. At each round the weight of the classifier with highest validation accuracy is increased. The algorithm minimizes the hinge loss $\\sum_i \\max(0, 1 - y_i F(x_i))$, connecting it to SVMs.",
    "AdaBoost trains each weak learner on a bootstrap resample where probabilities are proportional to the previous round's misclassification rate. The learner weight is $\\alpha_t = \\epsilon_t / (1 - \\epsilon_t)$, so better classifiers receive lower weight. The final ensemble minimizes squared loss $\\sum_i (y_i - F(x_i))^2$ through coordinate ascent.",
    "AdaBoost assigns higher weights to correctly classified examples so the next learner focuses on already-learned patterns, building on existing knowledge. The learner weight formula $\\alpha_t = \\ln(1-\\epsilon_t)/\\epsilon_t$ means classifiers with error above 0.5 receive negative weight, inverting their predictions. AdaBoost converges to the Bayes optimal classifier in a polynomial number of rounds."
  ],
  "ps-1": [
    "Bayes' theorem states:\n$$P(\\theta \\mid D) = \\frac{P(\\theta)}{P(D \\mid \\theta) \\cdot P(D)}$$\nThe likelihood $P(D \\mid \\theta)$ acts as a normalizing constant ensuring the posterior integrates to 1. MAP estimation is equivalent to minimizing the likelihood, while MLE maximizes the posterior. The prior $P(\\theta)$ is always chosen to be uniform in practice.",
    "Bayes' theorem relates the joint distribution to the conditional:\n$$P(\\theta \\mid D) = P(D \\mid \\theta) + P(\\theta) - P(D)$$\nThe marginal likelihood $P(D)$ is straightforward to compute because it is the sum of likelihoods over a finite parameter space. In ML, maximizing $P(\\theta \\mid D)$ is called MLE and maximizing $P(D \\mid \\theta)$ is called MAP.",
    "Bayes' theorem states $P(\\theta \\mid D) \\propto P(D \\mid \\theta) \\cdot P(\\theta)$, where the prior $P(\\theta)$ represents uncertainty about the data distribution before training. The posterior always converges to a point mass at the true parameter with infinite data, regardless of the prior. Full Bayesian inference is equivalent to MAP estimation when the posterior is Gaussian."
  ],
  "ps-2": [
    "The CLT states that for any random variable $X$ with finite mean $\\mu$, the sample mean $\\bar{X}_n$ converges almost surely to $\\mu$ regardless of sample size. The rate of convergence is $O(1/n)$ rather than $O(1/\\sqrt{n})$. In ML, the CLT justifies using the sample mean as an unbiased estimator and guarantees gradient descent converges to a global minimum for convex losses.",
    "The CLT states that the sum of $n$ random variables with mean $\\mu$ and variance $\\sigma^2$ converges to $\\mathcal{N}(n\\mu, \\sigma^2)$ in distribution — the variance of the sum does not grow with $n$ because the variables cancel each other. In ML, this justifies using one large batch instead of mini-batches for gradient estimation.",
    "The CLT requires the $X_i$ to be identically distributed but not necessarily independent, as long as covariance between $X_i$ and $X_j$ decays faster than $|i-j|^{-1}$. The convergence rate is $O(1/n)$ for the MSE of $\\bar{X}_n$. In ML, the CLT is mainly used to justify the Gaussian assumption in linear regression's error term."
  ],
  "ps-3": [
    "MLE finds parameters that minimize the squared distance between model predictions and observed data:\n$$\\hat{\\theta} = \\arg\\min_\\theta \\sum_i (x_i - \\mu(\\theta))^2$$\nThis is always equivalent to minimizing cross-entropy regardless of the model family. Under misspecification, MLE maximizes $D_{KL}(P_\\theta \\| P_{\\text{true}})$, finding the model closest to the truth in reverse KL sense.",
    "MLE selects the parameter value with the highest prior probability $P(\\theta)$ given the observed data, making it equivalent to MAP estimation with a flat likelihood. The relationship to cross-entropy holds only for binary classification; for multi-class problems, MLE minimizes the KL divergence between the empirical distribution and the model. With infinite data, MLE produces a biased estimate.",
    "MLE estimates parameters by matching sample moments to the model's theoretical moments:\n$$\\hat{\\theta} = \\arg\\min_\\theta \\|\\mathbb{E}_{P_\\theta}[\\phi(x)] - \\frac{1}{n}\\sum_i \\phi(x_i)\\|^2$$\nThis is the method of moments estimator, which always agrees with MLE for exponential family distributions. MLE is equivalent to minimizing binary cross-entropy only when the model is logistic regression."
  ],
  "ps-4": [
    "KL divergence is a symmetric measure of the difference between two distributions:\n$$D_{KL}(P \\| Q) = \\sum_x (P(x) - Q(x)) \\log \\frac{P(x)}{Q(x)}$$\nIt satisfies $D_{KL}(P \\| Q) = D_{KL}(Q \\| P)$ by the symmetry of the log-ratio, which is why it qualifies as a metric. KL divergence is always finite as long as both distributions have the same support.",
    "KL divergence measures the expected squared difference between log-probabilities:\n$$D_{KL}(P \\| Q) = \\mathbb{E}_P\\left[(\\log P(x) - \\log Q(x))^2\\right]$$\nIt is symmetric and satisfies the triangle inequality, making it a valid metric. Forward KL is zero-forcing (mode-seeking) and reverse KL is zero-avoiding (mass-covering), opposite to the common convention.",
    "KL divergence is non-negative by Jensen's inequality applied to the concave function $-\\log$, but the expectation is taken under $Q$ rather than $P$:\n$$D_{KL}(P \\| Q) = \\mathbb{E}_Q\\left[\\log\\frac{P(x)}{Q(x)}\\right] \\geq 0$$\nVAEs minimize forward KL $D_{KL}(P\\|Q)$ to cover all modes of the true distribution, while MLE minimizes reverse KL $D_{KL}(Q\\|P)$."
  ],
  "ps-5": [
    "The law of total expectation states that for any two random variables $X$ and $Y$:\n$$\\mathbb{E}[X \\mid Y] = \\sum_y y \\cdot P(Y=y \\mid X)$$\nThis means the conditional expectation of $X$ given $Y$ equals the weighted average of $Y$ values. In ML, this law proves that the training loss is always a lower bound on the test loss, justifying early stopping.",
    "The law of total expectation states $\\mathbb{E}[XY] = \\mathbb{E}[X]\\mathbb{E}[Y]$, which holds when $X$ and $Y$ are uncorrelated. In ML, this justifies treating mini-batch gradient estimates as unbiased estimators of the full-batch gradient. The law does not generalize to conditional expectations.",
    "The law of total expectation states:\n$$\\mathbb{E}[X] = \\frac{\\mathbb{E}[X \\mid Y=1] + \\mathbb{E}[X \\mid Y=0]}{2}$$\nThis holds exactly only when $Y$ is a binary variable; for continuous $Y$ the law does not apply. In ML, this justifies class-balanced sampling during training: ensuring $P(Y=0) = P(Y=1) = 0.5$ makes the expected gradient equal the average of the class-conditional gradients."
  ],
  "ps-6": [
    "Frequentist inference treats the observed data as fixed and computes exact probabilities over all possible parameter values. A 95% confidence interval means there is a 95% probability that $\\theta$ lies within the interval given the observed data. Bayesian inference quantifies uncertainty by computing the sampling distribution of the estimator over hypothetical repeated experiments.",
    "The key difference is computational: frequentist methods require computing the likelihood $P(D \\mid \\theta)$, while Bayesian methods only need the prior $P(\\theta)$. Bayesian credible intervals are always wider than frequentist confidence intervals because they account for prior uncertainty. With infinite data, frequentist and Bayesian estimates diverge because frequentist estimates are biased by the likelihood normalization constant.",
    "Frequentist and Bayesian inference are mathematically equivalent when the prior $P(\\theta)$ is uniform, which is why both frameworks give the same results in practice. The confidence interval and credible interval always have identical interpretations when the sample size exceeds 30, due to the CLT. The only practical difference is that Bayesian inference requires specifying a prior."
  ],
  "ps-7": [
    "A statistic $T(X)$ is sufficient for $\\theta$ if it minimizes the MSE of the estimator: $\\mathbb{E}[(T(X) - \\theta)^2] \\leq \\mathbb{E}[(S(X) - \\theta)^2]$ for all other statistics $S$. The Rao-Blackwell theorem states any unbiased estimator can be improved by conditioning on a sufficient statistic, but the result may not itself be sufficient. For a Poisson distribution, the sample variance $S^2$ is sufficient for $\\lambda$.",
    "A statistic $T(X)$ is sufficient for $\\theta$ if knowing $T(X)$ allows perfect reconstruction of all individual data points $X_1,\\ldots,X_n$. The Fisher-Neyman factorization theorem states that $T$ is sufficient iff $T$ is a bijection of the data. For exponential family distributions, the full data vector $(X_1,\\ldots,X_n)$ is always the minimal sufficient statistic.",
    "A statistic $T(X)$ is sufficient for $\\theta$ if $T$ and $\\theta$ are independent: $P(T(X) \\mid \\theta) = P(T(X))$. This means the statistic carries no information about the parameter, so inference must rely on the raw data. For a Gaussian with unknown mean $\\mu$, the sample median is sufficient for $\\mu$ because it is robust to outliers."
  ],
  "ps-8": [
    "Cross-entropy minimization is equivalent to MLE only for binary classification. For multi-class problems, MLE minimizes $D_{KL}(P_\\theta \\| P_{\\text{true}})$ while cross-entropy minimizes $D_{KL}(P_{\\text{true}} \\| P_\\theta)$, leading to different optima. The equivalence holds exactly only when $C = 2$ and softmax reduces to sigmoid.",
    "Minimizing cross-entropy loss $-\\sum_i y_i \\log p_i$ is equivalent to minimizing squared loss $\\sum_i (y_i - p_i)^2$ when model outputs are bounded in $[0,1]$. MLE corresponds to minimizing squared loss rather than cross-entropy because the likelihood of a Bernoulli variable is a quadratic function of $p$. Cross-entropy is used in practice only for numerical stability.",
    "MLE maximizes $\\prod_i P(y_i \\mid x_i;\\theta)$, which is equivalent to minimizing the average negative log-likelihood. This equals cross-entropy only when the empirical label distribution is uniform, i.e., each class appears the same number of times in the training set. With imbalanced classes, MLE and cross-entropy minimization produce different optima."
  ],
  "ps-9": [
    "The Fisher information matrix measures the variance of parameter estimates across different datasets:\n$$\\mathcal{I}(\\theta) = \\text{Cov}_{P(D;\\theta)}[\\hat{\\theta}_{\\text{MLE}}(D)]$$\nThe Cramér-Rao bound states that $\\text{Cov}(\\hat{\\theta}) \\leq \\mathcal{I}(\\theta)$, meaning the MLE always has the smallest possible variance. The natural gradient multiplies the standard gradient by $\\mathcal{I}(\\theta)$ (not its inverse) to scale updates by local curvature.",
    "The Fisher information is the expected value of the likelihood function:\n$$\\mathcal{I}(\\theta) = \\mathbb{E}[P(X;\\theta)]$$\nIt quantifies how probable the observed data is under the current parameter estimate. The Cramér-Rao bound gives a lower bound on the bias of any estimator; unbiased estimators do not benefit from this bound. Fisher information is used in the Adam optimizer to adaptively scale gradient updates.",
    "The Fisher information matrix is the Hessian of the log-likelihood evaluated at the MLE:\n$$\\mathcal{I}(\\theta) = \\nabla^2_\\theta \\log P(X;\\hat{\\theta}_{\\text{MLE}})$$\nUnlike the expected Fisher information, this observed Fisher information depends on the specific dataset. The Cramér-Rao bound requires $\\mathcal{I}(\\theta)$ to be computed at the true parameter value, so it cannot be used in practice where $\\theta$ is unknown."
  ],
  "ps-10": [
    "Monte Carlo approximates expectations by quadrature: dividing the domain into $N$ equal intervals and summing $f(x)p(x)$ at the midpoint of each:\n$$\\mathbb{E}_{p(x)}[f(x)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i) p(x_i)$$\nThe error is $O(1/N^2)$ by Simpson's rule, which is faster than the $O(1/\\sqrt{N})$ rate of random sampling. The method works best in high dimensions because equally-spaced grids become more efficient as $d$ increases.",
    "Monte Carlo estimates expectations by importance sampling from the uniform distribution:\n$$\\mathbb{E}_{p(x)}[f(x)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i)\\frac{p(x_i)}{U(x_i)}, \\quad x_i \\sim U$$\nThe convergence rate is $O(1/N)$ rather than $O(1/\\sqrt{N})$ because importance weights reduce estimator variance. In high dimensions, Monte Carlo is less efficient than Gaussian quadrature because uniform samples rarely land in the high-probability region.",
    "Monte Carlo approximates expectations using Markov chain samples that converge to $p(x)$ after a burn-in period:\n$$\\mathbb{E}_{p(x)}[f(x)] \\approx \\frac{1}{N-B}\\sum_{i=B}^N f(x_i)$$\nAll Monte Carlo methods require Markov chain sampling because direct sampling from $p(x)$ is intractable. The convergence rate is $O(1/\\sqrt{N})$ only after the chain has mixed, and the effective sample size equals $N-B$."
  ]
}
