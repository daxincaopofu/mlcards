// Raw seed decks — no initCard applied here; loadDecks() applies it.
export const SEED_DECKS = [
  {
    id: "deck-1", name: "ML Fundamentals", color: "#0ea5e9", icon: "⬡", created: Date.now(),
    cards: [
      { id: "c1", front: "What is the bias-variance tradeoff?", back: "Bias error comes from wrong assumptions in the learning algorithm (underfitting). Variance error comes from sensitivity to small fluctuations in training data (overfitting). Increasing model complexity reduces bias but increases variance. The goal is to find the sweet spot that minimizes total error." },
      { id: "c2", front: "Explain L1 vs L2 regularization.", back: "L1 (Lasso) adds $|w|$ penalty — produces sparse solutions by driving some weights to exactly zero; useful for feature selection. L2 (Ridge) adds $w^2$ penalty — shrinks weights toward zero but rarely exactly; handles multicollinearity well. The combined Elastic Net objective is:\n$$L = L_0 + \\lambda_1 \\|w\\|_1 + \\lambda_2 \\|w\\|_2^2$$" },
      { id: "c3", front: "What is the kernel trick in SVMs?", back: "The kernel trick implicitly maps data into a high-dimensional feature space by replacing dot products with $K(x, x') = \\phi(x) \\cdot \\phi(x')$. This allows SVMs to find non-linear decision boundaries without explicitly computing $\\phi$. Common kernels: RBF $K(x,x')=\\exp(-\\gamma\\|x-x'\\|^2)$, polynomial, and sigmoid." },
    ]
  },
  {
    id: "deck-2", name: "Deep Learning", color: "#f472b6", icon: "∇", created: Date.now(),
    cards: [
      { id: "c4", front: "What is batch normalization and why does it help?", back: "Batch normalization normalizes layer inputs to have zero mean and unit variance per mini-batch, then applies learnable γ (scale) and β (shift). Benefits: reduces internal covariate shift, allows higher learning rates, acts as slight regularization, reduces sensitivity to initialization." },
      { id: "c5", front: "Explain the attention mechanism in transformers.", back: "Attention computes a weighted sum of values based on query-key compatibility:\n$$\\text{Attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\nThe $\\sqrt{d_k}$ scaling prevents dot products from growing large in high dimensions. Multi-head attention runs this in parallel across $h$ subspaces, then concatenates and projects results." },
      { id: "c6", front: "Derive $\\partial L / \\partial z_j$ for cross-entropy loss with softmax.", back: "For true label $y$ (one-hot) and softmax output $p_i = e^{z_i}/\\sum_k e^{z_k}$:\n$$\\frac{\\partial L}{\\partial z_j} = p_j - y_j$$\nThis elegant result comes from the chain rule where $\\partial p_i/\\partial z_j = p_i(\\delta_{ij} - p_j)$. The softmax Jacobian and log loss cancel beautifully." },
      { id: "dl-1", front: "What is the vanishing gradient problem and how do residual connections address it?", back: "In deep networks, gradients diminish exponentially during backpropagation: $\\frac{\\partial L}{\\partial x} = \\prod_{i} \\frac{\\partial f_i}{\\partial x_{i-1}}$. When these Jacobians have spectral radius < 1, the product vanishes, making early layers nearly untrainable. Residual connections add a skip path $\\mathbf{y} = F(\\mathbf{x}) + \\mathbf{x}$, ensuring the gradient includes an identity term: $\\frac{\\partial L}{\\partial \\mathbf{x}} = \\frac{\\partial L}{\\partial \\mathbf{y}}\\!\\left(1 + \\frac{\\partial F}{\\partial \\mathbf{x}}\\right)$, which never vanishes." },
      { id: "dl-2", front: "How does dropout work and why does it reduce overfitting?", back: "During training, dropout randomly zeroes each neuron's output with probability $p$, then scales remaining activations by $\\frac{1}{1-p}$ to maintain expected values. This prevents co-adaptation: neurons cannot rely on specific others always being present, forcing each to learn more independent, robust features. At inference all neurons are active and no scaling is needed. Dropout approximates training an ensemble of $2^n$ thinned networks." },
      { id: "dl-3", front: "What are the key differences between LSTMs and GRUs?", back: "LSTMs have three gates (input, forget, output) and a separate cell state $c_t$ carrying long-term memory: $c_t = f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t$, $h_t = o_t \\odot \\tanh(c_t)$. GRUs simplify this with two gates (reset $r_t$, update $z_t$) and merge cell and hidden state: $h_t = (1-z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$. GRUs have fewer parameters, train faster, and perform comparably to LSTMs on most tasks." },
      { id: "dl-4", front: "Why do transformers need positional encodings, and how do sinusoidal encodings work?", back: "Self-attention is permutation-invariant: shuffling the input sequence produces the same attention weights. Positional encodings inject order information by adding a fixed vector to each token embedding. The original sinusoidal encoding uses:\n$$PE_{(pos,2i)} = \\sin\\!\\left(\\frac{pos}{10000^{2i/d}}\\right), \\quad PE_{(pos,2i+1)} = \\cos\\!\\left(\\frac{pos}{10000^{2i/d}}\\right)$$\nDifferent frequencies allow attention to relative and absolute positions; relative positions can be recovered via linear combinations of encodings." },
      { id: "dl-5", front: "What is knowledge distillation and how do soft targets improve learning?", back: "Knowledge distillation trains a small student network to mimic a large teacher's output distribution rather than hard labels. The student minimizes:\n$$L = \\alpha\\,L_{CE}(y,\\sigma(z_s)) + (1-\\alpha)\\,T^2\\,L_{CE}\\!\\left(\\sigma(z_t/T),\\sigma(z_s/T)\\right)$$\nwhere $T$ is temperature. High $T$ softens the teacher's distribution, exposing inter-class similarities that one-hot labels discard. This richer signal provides more gradient information per example, enabling a compact model to approach teacher-level accuracy." },
      { id: "dl-6", front: "What is the difference between generative and discriminative models?", back: "Discriminative models learn $P(y \\mid x)$ directly — the decision boundary separating classes (e.g., logistic regression, SVMs, neural classifiers). Generative models learn the joint $P(x, y) = P(x \\mid y)P(y)$, modeling how data is produced (e.g., naive Bayes, VAEs, GANs). Via Bayes' rule, $P(y \\mid x) = P(x \\mid y)P(y)/P(x)$, so generative models can classify, sample new data, handle missing features, and do density estimation — at the cost of modeling the often high-dimensional $P(x)$." },
      { id: "dl-7", front: "What is the reparameterization trick in Variational Autoencoders?", back: "VAEs encode input $x$ as a distribution $q_{\\phi}(z \\mid x) = \\mathcal{N}(\\mu, \\sigma^2)$ and must backpropagate through the sampling step. Direct sampling $z \\sim \\mathcal{N}(\\mu,\\sigma^2)$ is non-differentiable. The reparameterization trick writes $z = \\mu + \\sigma \\odot \\epsilon$, $\\epsilon \\sim \\mathcal{N}(0,I)$, moving randomness into a fixed distribution. Gradients now flow through $\\mu$ and $\\sigma$ normally, enabling end-to-end training of the ELBO:\n$$\\mathcal{L} = \\mathbb{E}_{q}[\\log p(x|z)] - D_{KL}(q_{\\phi}(z|x) \\| p(z))$$" },
      { id: "dl-8", front: "How does layer normalization differ from batch normalization, and when is each preferred?", back: "Batch norm normalizes across the batch dimension per feature: $\\hat{x}_i = (x_i - \\mu_B)/\\sigma_B$, requiring large batch sizes for stable statistics and using running stats at inference. Layer norm normalizes across the feature dimension per example: $\\hat{x} = (x - \\mu_L)/\\sigma_L$, making it batch-size independent. Layer norm is preferred for transformers and RNNs (variable sequence length, small batches); batch norm excels in CNNs processing fixed-size inputs with large batches." },
      { id: "dl-9", front: "What is gradient clipping and why is it used in training deep networks?", back: "Gradient clipping caps the gradient norm before the optimizer step. Global norm clipping rescales all gradients when their $\\ell_2$ norm exceeds threshold $\\tau$:\n$$g \\leftarrow g \\cdot \\frac{\\min(\\|g\\|,\\,\\tau)}{\\|g\\|}$$\nThis prevents the exploding gradient problem, where large Jacobian products cause updates to overshoot and destabilize training. It is especially important for RNNs and transformers on long sequences. Value clipping (clamping each element independently) is an alternative but can distort gradient direction." },
      { id: "dl-10", front: "Explain transfer learning and common strategies for fine-tuning pretrained models.", back: "Transfer learning reuses a model pretrained on a large source task for a smaller target task. Pretrained weights encode general features (edges, textures, syntax) transferable across domains. Common strategies: (1) **Feature extraction** — freeze all pretrained layers, train only a new head; (2) **Full fine-tuning** — update all weights on the target task; (3) **Gradual unfreezing** — unfreeze layers top-to-bottom progressively; (4) **PEFT** — add small trainable modules (LoRA, adapters) while keeping base weights frozen. Use a lower learning rate ($10^{-4}$ to $10^{-5}$) to preserve pretrained knowledge." },
    ]
  },
  {
    id: "deck-3", name: "Classical ML", color: "#4ade80", icon: "∑", created: Date.now(),
    cards: [
      { id: "cml-1", front: "Explain the Expectation-Maximization (EM) algorithm.", back: "EM is an iterative algorithm for maximum likelihood estimation with latent variables. Given observed data $X$ and latent variables $Z$, it maximizes $\\log P(X;\\theta)$ by alternating: **E-step** — compute posterior $Q(Z) = P(Z \\mid X, \\theta^{\\text{old}})$; **M-step** — maximize $\\mathbb{E}_{Q}[\\log P(X,Z;\\theta)]$ over $\\theta$. Each iteration is guaranteed to non-decrease the log-likelihood because the ELBO is a tight lower bound at the E-step. Classic applications: Gaussian Mixture Models, HMMs, missing-data imputation." },
      { id: "cml-2", front: "How does the random forest algorithm work and what makes it effective?", back: "Random forests build an ensemble of $T$ decision trees, each trained on a bootstrap sample of the data. At each split only a random subset of $m \\approx \\sqrt{p}$ features is considered, decorrelating the trees. Predictions are aggregated by majority vote (classification) or averaging (regression). The ensemble variance is $\\rho\\sigma^2 + \\frac{1-\\rho}{T}\\sigma^2$, where $\\rho$ is pairwise tree correlation. Feature subsampling reduces $\\rho$, so adding more trees reduces variance while bias stays constant." },
      { id: "cml-3", front: "How does gradient boosting differ from bagging, and what does it optimize?", back: "Bagging trains trees independently on bootstrap samples and averages predictions — primarily a variance-reduction technique. Gradient boosting trains trees **sequentially**, each fitting the negative gradient (pseudo-residuals) of the loss w.r.t. the current ensemble:\n$$F_m(x) = F_{m-1}(x) + \\eta\\,h_m(x)$$\nwhere $h_m \\approx -\\partial L/\\partial F_{m-1}$. This is functional gradient descent in prediction space. Boosting primarily reduces bias, can optimize arbitrary differentiable losses, and is more prone to overfitting than bagging." },
      { id: "cml-4", front: "Explain Principal Component Analysis (PCA) and its relationship to the SVD.", back: "PCA finds orthogonal directions of maximum variance. For centered data $X \\in \\mathbb{R}^{n \\times p}$, the covariance is $C = \\frac{1}{n-1}X^TX$. Principal components are the eigenvectors of $C$, equivalently the right singular vectors of $X$: via SVD $X = U\\Sigma V^T$, the PCs are columns of $V$ and projected coordinates are $U\\Sigma$. Selecting the top $k$ components minimizes reconstruction error $\\|X - X_k\\|_F^2 = \\sum_{i>k}\\sigma_i^2$ (Eckart-Young theorem). Kernel PCA generalizes PCA to non-linear manifolds." },
      { id: "cml-5", front: "Describe the k-nearest neighbors algorithm and analyze its computational properties.", back: "KNN classifies a query $x$ by finding its $k$ nearest training points by distance and taking a majority vote. There is no training phase — KNN is a lazy learner storing all data. Prediction cost is $O(np)$ per query for $n$ examples and $p$ features. Decision boundaries can approximate any shape, but performance degrades in high dimensions as distances concentrate (curse of dimensionality). Optimal $k$ balances bias (large $k$, smooth boundary) and variance (small $k$, complex boundary)." },
      { id: "cml-6", front: "Compare Gini impurity and information gain as decision tree splitting criteria.", back: "Both measure node impurity for class distribution $\\{p_k\\}$. **Gini impurity**: $G = 1 - \\sum_k p_k^2$ (probability of misclassifying a random sample). **Information gain**: $\\Delta H = H(\\text{parent}) - \\sum_j w_j H(\\text{child}_j)$ where $H = -\\sum_k p_k \\log p_k$. Gini favors the most frequent class, is faster to compute (no log), and tends to isolate the largest class. Entropy is more sensitive to probability changes near 0 and 1. In practice they produce very similar trees; CART uses Gini, C4.5/ID3 use entropy." },
      { id: "cml-7", front: "What is the curse of dimensionality and how does it affect machine learning?", back: "In high dimensions data becomes exponentially sparse: the volume of an inscribed hypersphere relative to the unit hypercube shrinks to zero as $d \\to \\infty$. Almost all volume lies near the surface, so random points concentrate at similar distances — distance metrics lose discriminative power. Practically: $O(r^{-d})$ samples are needed to maintain density $r$; distance-based methods degrade; irrelevant features add noise. Remedies: dimensionality reduction (PCA), feature selection, regularization, sparse models." },
      { id: "cml-8", front: "Explain the naive Bayes classifier and when its independence assumption is acceptable.", back: "Naive Bayes applies Bayes' rule assuming all features are conditionally independent given the class:\n$$P(y \\mid x_1,\\ldots,x_p) \\propto P(y)\\prod_{j=1}^p P(x_j \\mid y)$$\nThis makes estimation tractable ($O(pc)$ parameters for $c$ classes) and training a single pass. Despite frequent violations in practice (e.g., word co-occurrences), the classifier can still rank classes correctly even with miscalibrated probabilities. Works well for text classification (multinomial NB) and spam filtering." },
      { id: "cml-9", front: "What is k-fold cross-validation and how should it be used for model selection?", back: "K-fold CV partitions data into $k$ equal folds. For each fold $i$, train on the other $k-1$ folds and evaluate on fold $i$; the estimate is $\\hat{R} = \\frac{1}{k}\\sum_{i=1}^k L_i$. This gives a nearly unbiased estimate of generalization error with lower variance than a single split. For model selection: (1) use CV to compare hyperparameters; (2) retrain the selected model on the **full** training set; (3) evaluate on a held-out test set. Conflating model selection and final evaluation leads to optimistic bias." },
      { id: "cml-10", front: "Describe the AdaBoost algorithm and its connection to exponential loss minimization.", back: "AdaBoost maintains a weight distribution over training examples, initially uniform. At each round $t$: (1) train weak learner $h_t$ minimizing weighted error $\\epsilon_t$; (2) compute $\\alpha_t = \\frac{1}{2}\\ln\\frac{1-\\epsilon_t}{\\epsilon_t}$; (3) reweight: $w_i \\leftarrow w_i \\exp(-\\alpha_t y_i h_t(x_i))$, normalized. Final classifier: $H(x) = \\text{sign}\\!\\left(\\sum_t \\alpha_t h_t(x)\\right)$. Friedman et al. showed AdaBoost is coordinate descent on the exponential loss $L = \\sum_i \\exp(-y_i F(x_i))$." },
    ]
  },
  {
    id: "deck-4", name: "Probability & Statistics", color: "#a78bfa", icon: "◈", created: Date.now(),
    cards: [
      { id: "ps-1", front: "State Bayes' theorem and explain its components in the context of machine learning.", back: "Bayes' theorem:\n$$P(\\theta \\mid D) = \\frac{P(D \\mid \\theta)\\,P(\\theta)}{P(D)}$$\n$P(\\theta)$ is the **prior** (belief before data); $P(D \\mid \\theta)$ is the **likelihood**; $P(D) = \\int P(D \\mid \\theta)P(\\theta)\\,d\\theta$ is the marginal likelihood (normalizing constant); $P(\\theta \\mid D)$ is the **posterior**. In ML: the prior encodes regularization, MAP estimation maximizes the posterior, and full Bayesian inference averages predictions over the posterior." },
      { id: "ps-2", front: "State the Central Limit Theorem and explain its relevance to machine learning.", back: "For i.i.d. variables $X_1,\\ldots,X_n$ with mean $\\mu$ and finite variance $\\sigma^2$, the standardized sample mean converges in distribution:\n$$\\frac{\\bar{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\xrightarrow{d} \\mathcal{N}(0,1)$$\nIn ML: justifies Gaussian approximations for parameter estimates, motivates confidence intervals for evaluation metrics, explains why mini-batch gradient estimates have approximately normal noise, and underpins the validity of hypothesis tests comparing model performance." },
      { id: "ps-3", front: "What is maximum likelihood estimation (MLE) and how does it relate to minimizing cross-entropy?", back: "MLE finds parameters maximizing the probability of observed data:\n$$\\hat{\\theta} = \\arg\\max_\\theta \\sum_i \\log P(x_i;\\theta)$$\nFor a classifier with softmax output, $\\log P(y \\mid x;\\theta) = \\sum_k y_k \\log p_k$, so maximizing log-likelihood over the dataset is identical to minimizing the average cross-entropy $-\\frac{1}{n}\\sum_i\\sum_k y_{ik}\\log p_{ik}$. Under model misspecification, MLE converges to the parameter minimizing $D_{KL}(P_{\\text{true}} \\| P_{\\theta})$." },
      { id: "ps-4", front: "Define KL divergence and explain its key properties and asymmetry.", back: "KL divergence measures how much $Q$ differs from reference $P$:\n$$D_{KL}(P \\| Q) = \\mathbb{E}_P\\!\\left[\\log\\frac{P(x)}{Q(x)}\\right] \\geq 0$$\nKey properties: (1) $D_{KL} \\geq 0$ with equality iff $P = Q$ (Gibbs' inequality); (2) **asymmetric**: $D_{KL}(P\\|Q) \\neq D_{KL}(Q\\|P)$ in general; (3) not a metric. Forward KL $D_{KL}(P_{\\text{true}}\\|Q)$ is zero-avoiding (covers all modes); reverse KL $D_{KL}(Q\\|P_{\\text{true}})$ is zero-forcing (mode-seeking). VAEs minimize reverse KL; MLE minimizes forward KL." },
      { id: "ps-5", front: "State the law of total expectation and give an ML application.", back: "The law of iterated expectations states:\n$$\\mathbb{E}[X] = \\mathbb{E}_Y[\\mathbb{E}[X \\mid Y]]$$\nML applications: (1) decomposing generalization error by data partition; (2) deriving the bias-variance decomposition $\\mathbb{E}[(\\hat{f}-f)^2] = \\text{Bias}^2 + \\text{Var}$; (3) showing the Bayes optimal predictor is $f^*(x) = \\mathbb{E}[Y \\mid X=x]$; (4) analyzing EM convergence via the expected complete-data log-likelihood." },
      { id: "ps-6", front: "What is the fundamental difference between frequentist and Bayesian inference?", back: "**Frequentist**: probability is the long-run frequency of events; parameters $\\theta$ are fixed unknowns. Inference produces point estimates (MLE) and confidence intervals: in 95% of repeated experiments the interval contains the true $\\theta$. **Bayesian**: probability quantifies degree of belief; $\\theta$ is a random variable. Inference produces the posterior $P(\\theta \\mid D)$ and credible intervals: $P(\\theta \\in I \\mid D) = 0.95$. Predictions integrate over uncertainty: $P(x^* \\mid D) = \\int P(x^* \\mid \\theta)P(\\theta \\mid D)\\,d\\theta$." },
      { id: "ps-7", front: "What is a sufficient statistic and why is it important?", back: "A statistic $T(X)$ is sufficient for $\\theta$ if the conditional distribution of data given $T$ does not depend on $\\theta$: $P(X \\mid T(X),\\theta) = P(X \\mid T(X))$. By the Fisher-Neyman factorization theorem, $T$ is sufficient iff $P(X;\\theta) = g(T(X),\\theta)\\cdot h(X)$. A sufficient statistic captures all information in the data relevant to estimating $\\theta$. For a Gaussian with known variance, $\\bar{X}$ is sufficient for $\\mu$; for Bernoulli, $\\sum X_i$ is sufficient for $p$." },
      { id: "ps-8", front: "Derive why minimizing cross-entropy loss is equivalent to MLE for classification.", back: "For dataset $\\{(x_i,y_i)\\}$ and model $P(y\\mid x;\\theta)$, MLE maximizes:\n$$\\log\\mathcal{L}(\\theta) = \\sum_i \\log P(y_i \\mid x_i;\\theta)$$\nFor a $C$-class classifier with softmax output $p_k$ and one-hot $y_i$:\n$$\\log P(y_i \\mid x_i;\\theta) = \\sum_k y_{ik}\\log p_k$$\nMaximizing $\\sum_i \\log P(y_i \\mid x_i;\\theta)$ equals minimizing $-\\frac{1}{n}\\sum_i\\sum_k y_{ik}\\log p_{ik}$, the average cross-entropy $H(y,p)$. Cross-entropy training is exactly MLE under the categorical likelihood." },
      { id: "ps-9", front: "What is the Fisher information matrix and what does it quantify?", back: "The Fisher information matrix quantifies how much information data carries about $\\theta$:\n$$\\mathcal{I}(\\theta) = \\mathbb{E}\\left[\\nabla_\\theta \\log P(X;\\theta)\\,\\nabla_\\theta \\log P(X;\\theta)^T\\right] = -\\mathbb{E}\\left[\\nabla^2_\\theta \\log P(X;\\theta)\\right]$$\nThe Cramér-Rao bound states any unbiased estimator satisfies $\\text{Cov}(\\hat{\\theta}) \\geq \\mathcal{I}(\\theta)^{-1}$. In ML: $\\mathcal{I}^{-1}$ approximates the MLE covariance; the natural gradient $\\mathcal{I}^{-1}\\nabla L$ performs steepest descent in distribution space (used in natural gradient descent and TRPO)." },
      { id: "ps-10", front: "Explain the Monte Carlo method for computing expectations and why it scales well.", back: "Monte Carlo approximates intractable expectations by sampling:\n$$\\mathbb{E}_{p(x)}[f(x)] \\approx \\frac{1}{N}\\sum_{i=1}^N f(x_i), \\quad x_i \\sim p$$\nBy the law of large numbers the estimate converges almost surely; by CLT the error is $O(1/\\sqrt{N})$, **independent of dimension** — unlike quadrature rules that scale as $O(N^{-k/d})$. ML applications: Bayesian predictive distributions, VAE training via reparameterization, policy gradient estimation in RL (REINFORCE), and dropout as approximate model averaging." },
    ]
  },
];

export const DECK_COLORS = ["#0ea5e9", "#4ade80", "#f472b6", "#fb923c", "#a78bfa", "#34d399", "#f59e0b", "#60a5fa"];
export const DECK_ICONS = ["⬡", "∇", "∑", "◈", "⊕", "⟁", "⊗", "◉"];
export const QUALITY_BTNS = [
  { label: "Blackout", sub: "No recall", q: 0, color: "#ef4444" },
  { label: "Hard", sub: "Barely", q: 1, color: "#f97316" },
  { label: "Good", sub: "With effort", q: 2, color: "#eab308" },
  { label: "Easy", sub: "Instantly", q: 3, color: "#4ade80" },
];
